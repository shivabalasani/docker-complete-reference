------------------------------------------ Section 2 --------------------------------------------------------------------
Docker Commands :-
docker version
docker run hello-world

docker run busybox echo hi there
docker run busybox ls
docker run hello-world ls (error)

List all running containers: docker ps
docker run busybox ping google.com
List all containers that we have ever created : docker ps --all 

docker run = docker create + docker start

Create a Container : docker create <image name>
Start a Container : docker start <container id>

docker create hello-world (will generate id of the container that is created)
docker start -a 9271d.....21e (-a will watch for output from the container and prints it out on my terminal)

Restarting Stopped Containers : docker start -a 37a8c45a23e1
Removing Stopped Containers : docker system prune

Retrieving Log Outputs : dock logs <container id>
Stopping Containers : docker stop <container id> ( Terminate signal (SIGTERM) sent to running process and waits 10 seconds)
					  docker kill <container id> ( Kill signal (SIGKILL) sent to running process to terminate immediately)
					  

Multi-Command Containers : docker run redis						   
						   redis-server / redis-cli
						   
Executing Commands in Running Containers : docker exec -it <container id> <command>
										   docker exec -it 37a8c45a23e1 redis-cli
										   set myvalue 5 / get myvalue
										   
The purpose of the IT Flag : docker exec -it 37a8c45a23e1 redis-cli
							 docker exec -i -t 37a8c45a23e1 redis-cli (-i ensures any stuff that I type gets directed to STDIN of the running process.-t makes all the text show up a little bit pretty)
							 Every process in linux environment has STDIN/STDOUT/STDERR
							 docker exec -i 37a8c45a23e1 redis-cli

Getting a command prompt in a container : docker exec -it 37a8c45a23e1 sh ( Outputs #. Now can run any unix commands like cd /, ls. Ctrl + D or exit to exit)
										  In MacOS it is bash
										  In windows it is gitbash or powershell
										  In Unix zsh(Z shell)   
										  sh (shell)
										  
Starting with a shell : docker exec -it 37a8c45a23e1 sh
						docker run -it busybox sh ( Outputs #)
						
Container Isolation : docker run -it busybox sh (touch hithere to creates a new file. Multiple instances of the shell won't have the hithere file. Two running containers will have seperate file systems and data won't be shared)
										  
								
--------------------------------------------------------- Section 3 --------------------------------------------------------------------
Dockerfile - Configuration to define how our container should behave. Dockerfile -> Docker Client -> Docker Server -> Usable Image!
Creating a Dockerfile - Specify a base image -> Run some commands to install additional programs -> Specify a command to run on container startup.

Building a Dockerfile : Create an image that runs redis-server
						mkdir redis-image
						cd redis-image
												  (#User an existing docker image as a base / Download and install a dependency /  Tell the image what to do when it starts as a container)	
						code .  Create Dockerfile (FROM alpine 								/RUN apk add --update redis 		/ CMD ["redis-server"])
						docker build .
						docker run fc60771eaa08
						
Writing a dockerfile  == Being given a computer with no OS and being told to install Chrome.

Tagging an Image : docker build -t yourdockerid/RepoorProjectname:version .
				   docker build -t sbalasani/redis:latest .
				   docker run sbalasani/redis
				   
Manual Image generation with Docker Commit : docker run -it alpine sh
											 # apk add --update redis (Modifying file system)
							In new terminal: docker ps (get the id of the running container above)
											 docker commit -c 'CMD ["redis-server"]' fc60771eaa08
											 docker run 083589332af ( runs the image created above)


--------------------------------------------------------- Section 4 --------------------------------------------------------------------

Go to simpleweb
Build an image : docker build .

COPY 	./ 			./  ->(Place to copy stuff to inside *the container*)
		 |
		 \/
(Path to folder to copy from on *your machine* relative to build context)


Build an image with tag : docker build -t sbalasani/simpleweb .

Start the image : docker run sbalasani/simpleweb 
http://localhost:8080 (Error)

Docker run with Port Mapping : docker run -p 8080 : 8080 <image id>
					(Route incoming request to this port on local host to  : this port inside the container)
							   docker run -p 8080 : 8080 sbalasani/simpleweb

http://localhost:8080

Running shell inside the container : docker run -it sbalasani/simpleweb sh
		 					or 	   : docker exec -it eeed31ee65c6 sh (Should go to /usr/app #)
ls
exit

------------------------------------ Section 5 :  Docker Compose with Multiple Local Containers--------------------------------------------

Go to visits
Build an image : docker build .
Build an image with tag : docker build -t sbalasani/visits:latest .
Start the image : docker run sbalasani/visits 

Running Redis on a seperate container : docker run redis
Start the image : docker run sbalasani/visits 

Options for connecting Node App Container with Redis Container
1) Use Docker CLI's Networking Features
2) Use Docker Compose

Docker Compose 
	1) Seperate CLI that get installed along with Docker.
	2) Used to start up multiple Docker containers at the same time.
	3) Automates some of the long-winded arguments we were passing to 'docker run'

Docker Compose commands : docker-compose up   		(same as docker run myimage)
						  docker-compose up --build (same as  docker build . and docker run myimage)	

Stopping Docker Compose Containers : docker run -d redis	  (Lanunch single container)
									 docker stop ba1220ea9edc (Stop single container)

									 docker-compose up -d (Launch all containers in backgroud)
									 docker-compose stop  (Stop all containers)

Container Maintenance with Compose  
Automatic Container Restarts :  restart: always (add this to the container you want to restart in docker-compose.yaml)

	Status Codes :  0 - We exited and everthing is OK
				  1,2,3 etc - We exited because something went wrong!
	
	Restart Policies : "no" - Never attempt to restart this. container if it stops or crashes. Need to be in quotes in yaml file
						always - If this container stops *for any reason* always attempt to restart it.
						on-failure - Only restart if the container stops with and error code.
						unless-stoppped - Always restart unless we forcibly stop it.

Container Status with Docker Compose : docker-compose ps (Prints out the statuses of the containers that are running)
localhost:4001

------------------------------------ Section 6 :  Creating a Production-Grade Workflow--------------------------------------------
Instead of this:
npm install -g create-react-app
create-react-app frontend

Just do this:
npx create-react-app frontend

npm run start - Starts up a development server. For development use only
npm run test - Runs tests associated with the project
npm run build - Builds a production version of the application

Build an image using Dev Dockerfile : docker build -f Dockerfile.dev .
Duplicating dependencies : remove node_modules folder

Starting the container : docker run ba1220ea9edc (Error)
						 docker run -p 3000:3000 ba1220ea9edc

Create-React-App has some issues detecting when files get changed on Windows based machines.  To fix this, please do the following:
In the root project directory, create a file called .env
Add the following text to the file and save it: CHOKIDAR_USEPOLLING=true

Docker Volumes: set mapping from folder inside the container to the local folder so that changes are reflected immediately
				
				docker run -p 3000:3000 -v /app/node_modules -v $(pwd):/app <image_id>
				puts a bookmark on the node_modules folder and maps the present working directory(pwd) into the /app folder.
				use gitbash to run the above command.

				If you are on Windows, the command needs a minor change:
				docker run -p 3000:3000 -v /app/node_modules -v pwd:/app CONTAINER_ID

				Windows 10 Pro w/ Docker Desktop students have noted this variation to work with GitBash:
				docker run -p 3000:3000 -v /app/node_modules -v ${pwd}:/app CONTAINER_ID

Shorthand with Docker Compose : Add docker-compose.yaml 
								docker-compose up

								docker run ba1220ea9edc npm run test
								docker run -it ba1220ea9edc npm run test

Live updating tests			 :  Solution 1
								docker-compose up
								docker exec -t ba1220ea9edc npm run test

								Solution 2
								Update docker-compose.yaml to include tests service
								docker-compose up --build

								docker attach ba1220ea9edc ( Attach terminal to the containers stdin/stdout/stderr)
								docker exec -it ba1220ea9edc sh (Shell inside the container)
								# ps

Multi-Step Docker Builds : Create Dockerfile for production
Running Nginx			 : docker build . 
						   docker run -p 8080:80 ba1220ea9edc
localhost:8080


------------------------------------ Section 7 :  Continuous integration and deployment with AWS-------------------------------------------

Create .travis.yaml file and 
	1) Tell Travis we need a copy of docker running.
	2) Build our image using Dockerfile.dev.
	3) Tell Travis how to run our test suite.
	4) Tell Travis how to deploy our code to AWS.

git checkout -b feature
git add .
git push origin feature

Create and merge pull request from feature to master

------------------------------------ Section 8 :  Building a Multi-Container Application -------------------------------------------

------------------------------------ Section 9 :  Dockerizing Multiple Services ----------------------------------------------------
Go to complex
cd client
Build an image using Dev Dockerfile:  docker build -f Dockerfile.dev .
Run the container : 				  docker run ba1220ea9edc

cd server
Build an image using Dev Dockerfile:  docker build -f Dockerfile.dev .
Run the container : 				  docker run ba1220ea9edc

cd worker
Build an image using Dev Dockerfile:  docker build -f Dockerfile.dev .
Run the container : 				  docker run ba1220ea9edc

Go to complex-elastic-beanstalk
docker-compose up --build

------------------------------------ Section 10 :  Continuous Integration Workflow for Multiple Images -------------------------------
Add .travis.yaml file to push images to docker hub

------------------------------------ Section 11 :  Multi-Container Deployments to AWS ------------------------------------------------

------------------------------------ Section 12 :  Kubernetes ------------------------------------------------------------------------

Development - minikube
Production(Managed Solutions ) - Amazon Elastic Containe Service for Kubernetes (EKS)
							   - Google Cloud Kubernetes Engine (GKE)

minikube - Use for managing the VM itself
kubectl - Use for managing containers in the node

Local Kubernetes Development : Intall kubectl	- cli for interacting with our master
							   Install a VM driver virtualbox - used to make a VM that will be your single node
							   Install minikube - Runs a single node on that VM

kubectl version
minikube status
kubectl cluster-info


Docker Compose 															Kubernetes
Each entry can optionally get docker-compose to build an image 			Kubernetes expects all images to already be built
Each entry represents a conainter we want to create						One config file per object we want to create
Each entry defines the Networking requirements (ports)				 	We have to manually set up all networking


Get a simple conainter running on our local Kubernetes Cluster running
Make sure our image is hosted on docker hub
Make on config file to create the container
Make one config file to setup networking

Pods - Runs one or more closely related containers
Services - Sets up networking in a Kubernetes Cluster
		 - ClusterIP, Nodeport, LoadBalancer, Ingress
		 - NodePort Exposes a container to the outside world (only good for dev purposes)

go to simplek8s
kubectl apply -f <filename>

kubectl apply -f client-pod.yaml
kubectl apply -f client-node-port.yaml

Print the status of all running ports : kubectl get pods
										kubectl get services

minikube ip
http://192.168.99.100:31515


Kubernetes is a system to deploy containerized apps.
Nodes are individual machines (or vm's) that run containers.
Masters are machines (or vm's) with a set of programs to manage nodes.
Kubernetes didn't build our images - it got them from somewhere else.
Kubernetes (the master) decided where to run each container - each nodecan run a dissimilar set of containers.
To deploy something, we update the desired state of the master with a config file.
The master works constantly to meet your desired state.

------------------------------------------ Section 13 :  Maintaining sets of containers with deployments ---------------------------------------

Imperative approach :
Run a command to list out current running pods.
Run a command to update the current pod to use a new image.

Declarative approach:
Update our config file that originally created the pod.
Throw the updated config file into kubectl


go to simplek8s update client-pod.yaml image to multi-worker
kubectl apply -f client-pod.yaml

Get detailed info about an object : kubectl describe <object type> <object name>
									kubectl describe pod client-pod

update containerPort to 9999
kubectl apply -f client-pod.yaml //Error: Pod updates may not change fields other than image.


Object Types
Pods - Runs one or more closely related containers
Services - Sets up networking in a Kubernetes Cluster
Deployment - Maintains a set of identical pods, ensuring that they have the correct config and that the right number exists

Pods : Runs a single set of containers
	   Good for one-off dev purposes
	   Rarely used directly in production

Deployment :  Runs a set of identical pods (one or more)
			  Monitors the state of each pod, updating as necessary
			  Good for dev
			  Good for production

Create client-deployment.yaml
Remove an Object : kubectl delete -f <config file>
				   kubectl delete -f client-pod.yaml

kubectl get pods
kubectl apply -f client-deployment.yaml 
kubectl get deployments

minikube ip
http://192.168.99.100:31515

kubectl get pods -o wide
kubectl describe pods

Update Image Version : Change deployment to use multi-client again
					   Update the multi-client image
					   Tag the image with a version number, push to docker hub
					   Run a 'kubectl' command forcing the deployment to use the new image version

Change App.js in multi-client image.
docker build -t sbalasani/multi-client .
docker push sbalasani/multi-client
kubectl apply -f client-deployment.yaml (File is unchanged)

Manually delete pods to get the deployment to recreate them with the latest version - Deleting pods manually seems silly
Tag built images with a real version number and specify that version in the config file  - Adds an extra step in the production deployment process
Use an imperative command to update the image version the deployment should use - Uses an imperative command

cd complex
cd client
docker build -t sbalasani/multi-client:v5 .
docker push sbalasani/multi-client:v5 ( push updated version to docker hub)

cd simplek8s
kubectl set image <object_type> / <object_name> <container_name> = <new image to use>
kubectl set image deployment/client-deployment client=sbalasani/multi-client:v5

kubectl get pods
minikube ip
http://192.168.99.100:31515

Reconfiguring Docker CLI:
docker ps
Configure the VM to Use Your Docker Server : eval $(minikube docker-env)
minikube docker-env
docker ps

This only configures your current terminal window.You can look up how to run a command in your terminal every time you open a window to fix this.


------------------------------------------ Section 14 :  A Multi-Container App with Kubernetes -----------------------------------------------------
cd complex
docker-compose up --build

Object Types
Pods - Runs one or more closely related containers
Services - Sets up networking in a Kubernetes Cluster
		 - ClusterIP : Exposes a set of pods to other objects in the cluster
		 - NodePort : Exposes a set of pods to the outside world (only good for dev purposes!!!)
		 - LoadBalancer
		 - Ingress

kubectl get deployments
kubectl delete deployment client-deployment
kubectl get services
kubectl delete service client-node-port

Add client-deployment.yaml
Add client-cluster-ip-service.yaml
Add server-deployment.yaml
Add server-cluster-ip-service.yaml
Add worker-deployment.yaml
Add redis-deployment.yaml
Add redis-cluster-ip-service.yaml
Add postgres-deployment.yaml
Add postgres-cluster-ip-service.yaml

kubectl apply -f k8s
kubectl get pods
kubectl get deployments
kubectl get services

"Volume" in generic container terminology : Some type of mechanism that allows a container to access a filesystem outside itself
"Volume" in Kubernetes : An object that allows a container to store data at the pod level

Access Modes
	ReadWriteOnce - Can be used by a single node.
	ReadOnlyMany - Multiple nodes can read from this.
	ReadWriteMany - Can be read and written to by many nodes

kubectl get storageclass
kubectl describe storageclass

kubectl get pv
kubectl get pvc (persistent volume claims)

Creating a secret : kubectl create secret generic <secret_name> --from-literal key=value
					kubectl create secret generic pgpassword --from-literal PGPASSWORD=1234asdf

kubectl get secrets

------------------------------------------ Section 15 :  Handling Traffic with Ingress Controllers -----------------------------------------------------

Ingress - Exposes a set of services to the outside world
go to complex

https://kubernetes.github.io/ingress-nginx/deploy/
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/mandatory.yaml
minikube addons enable ingress
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/provider/cloud-generic.yaml

https://192.168.99.100

minikube dashboard

------------------------------------------ Section 16 :  Kubernetes Production Deployment -----------------------------------------------------
Create muli-k8s git hub repository
Create local repo . go to complex
git remote -v 
git remote remove origin
git remote add origin

Create google cloued project :  console.cloud.google.com
Create Kubernetes Cluster in google cloud.

Travis Config File
	- Install Google Cloud SDK CLI
	- Configure the SDK with out Google Cloud auth info
	- Login to Docker CLI
	- Build the 'test' version of multi-client
	- Run tests
	- If tests are successful, run a script to deploy newest images
	- Build all our images, tag each one, push each to docker hub
	- Apply all configs in the 'k8s' folder
	- Imperatively set latest images on each deployment

Create a Service Account
Download service account credentials in a json file
Download and install the Travis CLI
Encrypt and upload the json file to our Travis account
In travis.yml, add code to unencrypt the json file and load it into GCloud SDK

go to complex
Download and install the Travis CLI : docker run -it -v $(pwd):/app ruby:2.3 sh
									  cd app
									  # gem install travis --no-rdoc --no-ri
									  # gem install travis
									  # travis login
									  Copy json file into the 'volumed' directory so we can use it in the container
									  travis encrypt-file service-account.json -r shivabalasani/multi-k8s

git rev-parse HEAD
git log

kubectl create secret generic pgpassword --from-literal PGPASSWORD=mypgpassword123 

https://github.com/helm/helm
https://helm.sh/docs/intro/quickstart/

In google cloud cli
$ curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3
$ chmod 700 get_helm.sh
$ ./get_helm.sh

Role Based Access Control (RBAC)
		- Limits who can access and modify objects in our cluster
		- Enabled on Google Cloud by default
		- Tiller wants to make changes to our cluster, so it needs to get some permissions set

kubectl get namespaces

Assigning Tiller a Service Account
  kubectl create serviceaccount --namespace kube-system tiller 
  		- Create a new service account called tiller in the kube-system namespace
  kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller
		-Create a new clusterrolebinding with the role 'cluster-admin' and assign it to service account 'tiller'

 helm init --service-account tiller --upgrade
 helm install my-nginx stable/nginx-ingress --set rbac.create=true

 ------------------------------------------ Section 17 :  HTTPS Setup with Kubernetes -----------------------------------------------------

 Purchase domain : domains.google.com
 github.com/jetstack/cert-manager
 helm install \
	--name cert-manager \
	--namespace kube-system \
	stable/cert-manager

create issuer.yaml
create certificate.yaml

git checkout master

kubectl get certificates
kubectl describe certificates
kubectl get secrets

 ------------------------------------------ Section 18 : Local Development with skaffold -----------------------------------------------------

 brew install skaffold
 skaffold version

 create skaffold.yaml in complex
 skaffold dev